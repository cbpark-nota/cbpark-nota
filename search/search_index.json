{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NetsPresso Documentation","text":"<p>Nota has built NetsPresso to help other developers to make custom AI models faster and easier for edge devices. NetsPresso aims to provide critical features to offer hands-on experience with hardware-aware autoML, optimization, and device farm to explore possibilities with NetsPresso.</p> <p>This document includes product configuration, benchmarks, and other important information to help you during using NetsPresso. Please feel free to compare the results obtained using Nota NetsPresso with other model architecture optimization frameworks. We are constantly striving to keep up with the latest AI model optimization technology and we welcome your feedback!</p>"},{"location":"#about-nota","title":"About Nota","text":"<p>In 2015, we started with a mobile keyboard app that prevents typos. During the app development, we realized how painful it was to optimize the deep learning model on a tiny mobile device. That experience lead us to the decision to share our know-hows with other developers not only to build, but optimize models per a particular hardware. Since then, Nota has focused on deep learning optimization technology and will continue to develop its AI software optimization platform to support AI developer community. Nota has offices in South Korea, US, and Germany and partners with over 35 global market leaders from a wide range of industries including system integrator, device manufacturers, platform providers, and mobility.</p> <p>\ud83c\udfe0\u00a0More about Nota</p>"},{"location":"#contact-us","title":"Contact us","text":"<p>If you have any questions or comments, please contact us.</p> <p>We look forward to your feedback.</p> <p>\ud83d\ude4f Leave Feedbacks:\u00a0NetsPresso Discussion Forum</p> <p>\ud83d\udc68\u200d\ud83d\udcbb NetsPresso Support Center:\u00a0netspresso@nota.ai</p>"},{"location":"en/launchx/","title":"LaunchX","text":"<p>LaunchX is a web application that facilitates the essential tasks of framework conversion and benchmarking for deploying AI models on devices quickly and easily. It allows AI developers, researchers, and even non-coders to intuitively use the ultimate converter and benchmarker. If you are a Python user, you can access the same functionality as a Python package through NetsPresso.</p>"},{"location":"en/launchx/#models","title":"Models","text":"<p>We provide optimized models through NetsPresso. You can immediately utilize these AI models after converting them.</p> <ul> <li>Utilize a variety of computer vision task models for free.</li> <li>You can download converted models.</li> <li>You can request customization for a fine-tuned model that is tailored to your project environment.</li> </ul>"},{"location":"en/launchx/#convert","title":"Convert","text":"<p>Simplify the process of framework conversion.</p> <p></p> <ul> <li>Easy and efficient model conversion.</li> <li>Eliminate trial and error with automated conversion.</li> <li>Quickly set up the deployment of models on target devices.</li> </ul> <p>TODO: link change See supported frameworks</p>"},{"location":"en/launchx/#benchmark","title":"Benchmark","text":"<p>Easily find the right device for your AI model.</p> <p></p> <ul> <li>Identify the best-fit device through benchmarking.</li> <li>Check model compatibility with target devices.</li> <li>Evaluate real-device performance without acquiring or setting up the device.</li> </ul> <p>TODO: link change View compatible devices</p>"},{"location":"en/launchx/#request-your-device","title":"Request your device","text":"<p>We continuously research and develop to optimize AI models whenever new devices are released through partnerships with device manufacturers. If you have a specific device you'd like to benchmark, please make a request to us.</p>"},{"location":"en/launchx/#we-hear-from-you","title":"We hear from you","text":"<p>If you encounter difficulties or need additional credits, feel free to contact us anytime!</p> <p>\ud83d\udc68\u200d\ud83d\udcbb NetsPresso Support Center:\u00a0netspresso@nota.ai</p>"},{"location":"en/launchx/compatibility_range_of_benchmark/","title":"Compatibility range of benchmark","text":"<p>You can measure the actual performance on the devices listed below without the need to purchase or install the device.</p> <p>Supported Hardware</p> <p></p> <p>Arm MCU/NPU</p> Target Device .tflite Renesas RA8D1 (Arm Cortex-M85) O (only INT8) Renesas RA8D1 (Arm Cortex-M85) with helium O (only INT8) Alif Ensemble DevKit-E7 Gen2 (Arm Cortex-M55 + Ethos-U55) O (only INT8) Alif Ensemble DevKit-E7 Gen2 (Arm Cortex-M55 + Ethos-U55) with helium O (only INT8) Arm Virtual Hardware Corstone-300(Ethos-U55/U65) O (only INT8) Arduino Nicla Vision(Arm Cortex-M7/M4) O (only INT8) NXP i.MX 93(Arm Cortex-A55/M33+Ethos-U65) O (only INT8) <p>NVIDIA \u2022 When benchmarking on Jetson, it is essential for the model file and target device to match the Jetpack version.</p> Target Device .trt .engine .tflite .onnx Jetson Nano JetPack 4.4.1 O O X O Jetson Nano JetPack 4.6 O O X O Jetson Xavier NX JetPack 4.6 O O X O Jetson Xavier NX JetPack 5.0.2 O O X O Jetson TX2 JetPack 4.6 O O X O Jetson AGX Xavier JetPack 4.6 O O X O Jetson AGX Orin JetPack 5.0.1 O O X O Jetson Orin Nano JetPack 6.0 O O X O AWS-T4 O O X O <p>Raspberry Pi</p> Target Device .tflite .onnx Raspberry Pi ZeroW O X Raspberry Pi Zero2W O X Raspberry Pi 2B O O Raspberry Pi 3B O O Raspberry Pi 3B+ O O Raspberry Pi 4B O O Raspberry Pi 5 O O <p>Intel</p> Target Device .zip(bin+xml) Xeon W-2223 O"},{"location":"en/launchx/compatibility_range_of_conversion/","title":"Compatibility range of conversion","text":"<p>You can use LaunchX converter to automatically convert the AI model's framework to the target framework.</p>"},{"location":"en/launchx/compatibility_range_of_conversion/#converting-case","title":"Converting case","text":"<ol> <li>ONNX to TensorRT</li> <li>ONNX to TensorFlow Lite</li> <li>ONNX to OpenVINO</li> <li>TensorFlow-Keras to TensorFlow Lite</li> </ol>"},{"location":"en/launchx/compatibility_range_of_conversion/#compatible-model","title":"Compatible model","text":"<p>The input layer of the uploaded model should be as follows.</p> <p></p> <ul> <li>Only single-input models are supported.</li> <li>The four-dimensional array structure of images should be organized Batch, Number of Channels, Height, and Width.</li> </ul> <ol> <li>Batch size: The number of combined input datasets that the model processes simultaneously.</li> <li>Channel: 3 for RGB or BGR and 1 for Grayscale.</li> <li>Input size: In computer vision tasks, input size refers to the size of the input images. </li> </ol>"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-tensorrt","title":"ONNX to TensorRT","text":"<p>Supported JetPack-ONNX version</p> Target Device JetPack version Input datatype Batch size Channel Input size Output datatype NVIDIA Jetson Nano 4.6, 4.4.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Xavier NX 5.0.2, 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson TX2 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Xavier 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Orin 5.0.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Orin Nano 6.0 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA T4 - FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-tflite","title":"ONNX to TFlite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16INT8"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-openvino","title":"ONNX to OpenVino","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"en/launchx/compatibility_range_of_conversion/#tensorflow-to-tensorflowlite","title":"TensorFlow to TensorFlowLite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static, Dynamic) 1~4 height, width FP16INT8"},{"location":"en/launchx/int8_quantization_with_launchx/","title":"INT8 quantization with LaunchX","text":""},{"location":"en/launchx/int8_quantization_with_launchx/#what-is-quantization","title":"What is Quantization?","text":"<p>Quantization is a technique that reduces the number of bits used to represent weights and activations in a deep learning model. This makes the model lighter, reducing memory usage and improving inference speed.</p> <p>LaunchX facilitates the easy application of post-training INT8 quantization to the model. It represents the model's weights and activations as 8-bit integers, saving memory and accelerating computations.</p> <p>This document provides information as follows:</p> <ul> <li>INT8 quantization with LaunchX</li> <li>Preparing the calibration dataset</li> <li>Inference code for TFlite INT8 model</li> </ul>"},{"location":"en/launchx/int8_quantization_with_launchx/#int8-quantization-with-launchx_1","title":"INT8 quantization with LaunchX","text":"<ol> <li>To quantize, upload your target model or select from the free models in Models.</li> <li>On the Convert Setting page, choose Tensorflow Lite, select the target device, and then choose the INT8 option.</li> <li>Upload the calibration dataset(&lt;= 500MB) to minimize quantization error.</li> <li>Click the convert button.</li> </ol>"},{"location":"en/launchx/int8_quantization_with_launchx/#preparing-the-calibration-dataset","title":"Preparing the calibration dataset","text":"<p>Calibration is the process of aligning a quantized model with a specific data distribution, adjusting quantization parameters such as scaling and zero points.</p> <p>Generally, the calibration dataset is a subset of the original training data, used to fine-tune quantization parameters. Its purpose is to ensure the quantized model's optimal performance on real-world data while maintaining acceptable accuracy. During calibration, the model runs on this dataset, and activation statistics are collected. These statistics determine quantization parameters for each neural network layer, minimizing the impact of quantization on accuracy by adapting parameters based on observed value distributions.</p>"},{"location":"en/launchx/int8_quantization_with_launchx/#convert-the-dataset-to-numpy-for-calibration","title":"Convert the dataset to NumPy for calibration.","text":"<ul> <li>img_file_dir_path : Path where the dataset for calibration is stored.</li> <li>padding : Value for padding preprocessing.</li> <li>color_mode : Value to change the color mode of an image for preprocessing. Select \u2018bgr\u2019 or \u2018rgb\u2019.</li> <li>normalize : Value for normalization preprocessing.</li> <li>input_shape : Value for width and height of model input shape.</li> <li>save_file_path : Path to save the npy file.</li> </ul> <pre><code>from glob import glob\nimport os\nimport pathlib\nimport random\nfrom typing import Literal, List, Tuple\n\nimport cv2\nimport numpy as np\n\nimg_file_types = [\n    \"*.jpeg\",\n    \"*.JPEG\",\n    \"*.jpg\",\n    \"*.JPG\",\n    \"*.png\",\n    \"*.PNG\",\n    \"*.BMP\",\n    \"*.bmp\",\n    \"*.TIF\",\n    \"*.tif\",\n    \"*.TIFF\",\n    \"*.tiff\",\n    \"*.DNG\",\n    \"*.dng\",\n    \"*.WEBP\",\n    \"*.webp\",\n    \"*.mpo\",\n    \"*.MPO\",\n]\n\ndef save_npy_file(array, save_file_name: str):\n    np.save(save_file_name, array)\n\ndef print_saved_npy_file_shape(save_file_name: str):\n    array = np.load(save_file_name)\n    print(f\"Saved npy array shape is {array.shape}\")\n\ndef save_txt_file(lines: List[str], file_name: str):\n    with open(file_name, \"w\") as file:\n        for line in lines:\n            file.write(line + \"\\n\")\n        file.close()\n\ndef print_error_files(error_files: List[str], file_name: str):\n    print(f\"Please check files in {file_name}.\")\n    print(\"Error files:\", error_files)\n\ndef save_error_files(\n        error_files: List[str],\n        file_name: str = \"error_files.txt\"):\n    if len(error_files) != 0:\n        save_txt_file(error_files, file_name)\n    print_error_files(error_files, file_name)\n\nclass DataPreprocessor():\n    def __init__(\n        self,\n        root_dir: str,\n        input_shape: Tuple[int, int],\n        color_mode: Literal[\"bgr\", \"rgb\"],\n        padding: bool,\n        normalize: bool\n    ):\n        self.root_dir = root_dir\n        self.input_shape = input_shape\n        self.color_mode = color_mode\n        self.padding = padding\n        self.normalize = normalize\n        self.img_list = []\n\n    def _get_img_list(self,):\n        self.img_list = []\n\n        for root, dirs, files in os.walk(self.root_dir):\n            for img_type in img_file_types:\n                img_files = glob(os.path.join(root, img_type))\n                self.img_list += img_files\n\n        self.img_list = list(set(self.img_list))\n        if len(self.img_list) == 0:\n            raise Exception(f\"No image in {self.root_dir}\")\n\n        random.shuffle(self.img_list)\n\n    def _img_padding(self, im):\n        shape = im.shape[:2]  # current shape [height, width]\n        new_shape = self.input_shape\n        if isinstance(new_shape, int):\n            new_shape = (new_shape, new_shape)\n\n        # Scale ratio (new / old)\n        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n\n        # Compute padding\n        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n        if shape[::-1] != new_unpad:  # resize\n            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n        left, right = int(round((new_shape[1] - new_unpad[0])/2 - 0.1)), int(round((new_shape[1] - new_unpad[0])/2 + 0.1))\n        top, bottom = int(round((new_shape[0] - new_unpad[1])/2 - 0.1)), int(round((new_shape[0] - new_unpad[1])/2 + 0.1))\n        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114,114,114))  # add border\n        return im\n\n    def _preprocess(self, img):\n        x = []\n        if self.color_mode == \"rgb\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.padding:\n            img = self._img_padding(img)\n        else:\n            img = cv2.resize(img, self.input_shape)\n        x.append(np.asarray(img))\n        x = np.array(x)\n        x = np.float32(x)\n        if self.normalize:\n            x = x / 255.0\n        return x\n\n    def _load_dataset(self,):\n        error_files = []\n        valid_images = []\n\n        def is_valid_image(image):\n            return image is not None\n\n        for f in self.img_list:\n            try:\n                img = self._preprocess(cv2.imread(f))\n                if is_valid_image(img):\n                    valid_images.append(img)\n                else:\n                    error_files.append(f)\n            except Exception as e:\n                error_files.append(f)\n        try:\n            result_array = np.concatenate(valid_images, axis=0)\n        except:\n            import pdb; pdb.set_trace()\n        return result_array, error_files\n\n    def _get_save_file_name(self,):\n        self._get_img_list()\n        count = len(self.img_list)\n        w = self.input_shape[0]\n        h = self.input_shape[1]\n        return f\"{count}x{w}x{h}.npy\"\n\n    def save_dataset_as_npy(self, save_file_path: str):\n        save_file_name = self._get_save_file_name()\n        save_file_path = os.path.join(save_file_path, save_file_name)\n        result_array, error_files = self._load_dataset()\n        save_npy_file(result_array, save_file_path)\n        print_saved_npy_file_shape(save_file_path)\n        save_error_files(error_files)\n\nif __name__ == \"__main__\":\n    img_file_dir_path = \"/image/file/path\"\n    padding = True\n    color_mode = \"bgr\"\n    input_shape = (512, 512)\n    normalize = True\n    save_file_path = \"/path/to/save/file\"\n    data_preprocessor = DataPreprocessor(\n        img_file_dir_path,\n        input_shape,\n        color_mode,\n        padding,\n        normalize\n    )\n    data_preprocessor.save_dataset_as_npy(save_file_path)\n</code></pre> <p>Result</p> <pre><code>root@4e54bd5645d1:/app# python3 prepare_npy_file.py \nCorrupt JPEG data: 122 extraneous bytes before marker 0xc4\nSaved npy array shape is (145, 32, 32, 3)\nPlease check files in error_files.txt.\nError files: ['/app/calib_dataset/images/2007_000039_jpg']\n</code></pre>"},{"location":"en/launchx/int8_quantization_with_launchx/#dataset-that-failed-to-be-read","title":"Dataset that failed to be read","text":"<p>Image files that do not open normally through opencv-python are listed in the 'error_files.txt' created in the code execution location.</p>"},{"location":"en/launchx/int8_quantization_with_launchx/#inference-code-for-tflite-int8","title":"Inference Code for TFlite INT8","text":"<p>Write and use additional pre-processing/post-processing codes for your model.</p> <pre><code>import os\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\ndef model_input_output_attributes(interpreter: tf.lite.Interpreter):\n    inputs = {}\n    outputs = {}\n    for input_detail in interpreter.get_input_details():\n        input_data_attribute = {}\n        input_data_attribute[\"name\"] = input_detail.get(\"name\")\n        input_data_attribute[\"location\"] = input_detail.get(\"index\")\n        input_data_attribute[\"shape\"] = tuple(input_detail.get(\"shape\"))\n        input_data_attribute[\"dtype\"] = input_detail.get(\"dtype\")\n        input_data_attribute[\"quantization\"] = input_detail.get(\"quantization\")\n        input_data_attribute[\"format\"] = \"nchw\" if input_data_attribute[\"shape\"][1] == 3 else \"nhwc\"\n        inputs[input_data_attribute[\"location\"] if input_data_attribute[\"location\"] is not None else input_data_attribute[\"name\"]] = input_data_attribute\n\n    for output_detail in interpreter.get_output_details():\n        output_data_attribute = {}\n        output_data_attribute[\"name\"] = output_detail.get(\"name\")\n        output_data_attribute[\"location\"] = output_detail.get(\"index\")\n        output_data_attribute[\"shape\"] = tuple(output_detail.get(\"shape\"))\n        output_data_attribute[\"dtype\"] = output_detail.get(\"dtype\")\n        output_data_attribute[\"quantization\"] = output_detail.get(\"quantization\")\n        output_data_attribute[\"format\"] = \"nchw\" if output_data_attribute[\"shape\"][1] == 3 else \"nhwc\"\n        outputs[output_data_attribute[\"location\"] if output_data_attribute[\"location\"] is not None else output_data_attribute[\"name\"]] = output_data_attribute\n\n    return inputs, outputs\n\ndef preprocess(input_image_path:str):\n    input_image = cv2.imread(input_image_path)\n    # Write down the pre-processing process required for your model, including resizing.\n    return input_image\n\ndef inference(tflite_model_path:str, input_image):\n    with open(os.path.join(tflite_model_path), \"rb\") as f:\n        interpreter = tf.lite.Interpreter(model_content=f.read(), num_threads=1)\n    interpreter.allocate_tensors()\n\n    inputs, outputs = model_input_output_attributes(interpreter)\n    for k, v in inputs.items():\n        if v['dtype'] in [np.uint8, np.int8]:\n            input_scale, input_zero_point = v[\"quantization\"]\n            input_image = input_image / input_scale + input_zero_point\n            input_image = np.expand_dims(input_image, axis=0).astype(v[\"dtype\"])\n        interpreter.set_tensor(v[\"location\"], input_image)\n    interpreter.invoke()\n\n    output_dict = {}\n    for output_location in iter(outputs):\n        output_dict[output_location] = interpreter.get_tensor(output_location)\n\n    return output_dict\n\ndef postprocess(inference_results):\n    return inference_results\n\ninput_image=preprocess(\"your_image_file.jpg\")\ninference_result=inference(\"your_model.tflite\", input_image)\npostprocess(inference_result)\n</code></pre>"},{"location":"en/launchx/supported_jetpack_onnx_version/","title":"Supported JetPack-ONNX version","text":"JetPack \bonnxruntime ONNX ONNX opset version ONNX ML opset version ONNX IR version 4.4 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.4.1 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.6 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 5.0.1 1.12.1 1.12 17 3 8 5.0.2 1.12.1 1.12 17 3 8 6.0 1.17.0 1.15 20 4 9"},{"location":"ko/launchx/compatibility_range_of_benchmark/","title":"Compatibility range of benchmark","text":"<p>You can measure the actual performance on the devices listed below without the need to purchase or install the device.</p> <p>Supported Hardware</p> <p></p> <p>Arm MCU/NPU</p> Target Device .tflite Renesas RA8D1 (Arm Cortex-M85) O (only INT8) Renesas RA8D1 (Arm Cortex-M85) with helium O (only INT8) Alif Ensemble DevKit-E7 Gen2 (Arm Cortex-M55 + Ethos-U55) O (only INT8) Alif Ensemble DevKit-E7 Gen2 (Arm Cortex-M55 + Ethos-U55) with helium O (only INT8) Arm Virtual Hardware Corstone-300(Ethos-U55/U65) O (only INT8) Arduino Nicla Vision(Arm Cortex-M7/M4) O (only INT8) NXP i.MX 93(Arm Cortex-A55/M33+Ethos-U65) O (only INT8) <p>NVIDIA \u2022 When benchmarking on Jetson, it is essential for the model file and target device to match the Jetpack version.</p> Target Device .trt .engine .tflite .onnx Jetson Nano JetPack 4.4.1 O O X O Jetson Nano JetPack 4.6 O O X O Jetson Xavier NX JetPack 4.6 O O X O Jetson Xavier NX JetPack 5.0.2 O O X O Jetson TX2 JetPack 4.6 O O X O Jetson AGX Xavier JetPack 4.6 O O X O Jetson AGX Orin JetPack 5.0.1 O O X O Jetson Orin Nano JetPack 6.0 O O X O AWS-T4 O O X O <p>Raspberry Pi</p> Target Device .tflite .onnx Raspberry Pi ZeroW O X Raspberry Pi Zero2W O X Raspberry Pi 2B O O Raspberry Pi 3B O O Raspberry Pi 3B+ O O Raspberry Pi 4B O O Raspberry Pi 5 O O <p>Intel</p> Target Device .zip(bin+xml) Xeon W-2223 O"},{"location":"ko/launchx/compatibility_range_of_conversion/","title":"Compatibility range of conversion","text":"<p>You can use LaunchX converter to automatically convert the AI model's framework to the target framework.</p>"},{"location":"ko/launchx/compatibility_range_of_conversion/#converting-case","title":"Converting case","text":"<ol> <li>ONNX to TensorRT</li> <li>ONNX to TensorFlow Lite</li> <li>ONNX to OpenVINO</li> <li>TensorFlow-Keras to TensorFlow Lite</li> </ol>"},{"location":"ko/launchx/compatibility_range_of_conversion/#compatible-model","title":"Compatible model","text":"<p>The input layer of the uploaded model should be as follows.</p> <p></p> <ul> <li>Only single-input models are supported.</li> <li>The four-dimensional array structure of images should be organized Batch, Number of Channels, Height, and Width.</li> </ul> <ol> <li>Batch size: The number of combined input datasets that the model processes simultaneously.</li> <li>Channel: 3 for RGB or BGR and 1 for Grayscale.</li> <li>Input size: In computer vision tasks, input size refers to the size of the input images. </li> </ol>"},{"location":"ko/launchx/compatibility_range_of_conversion/#onnx-to-tensorrt","title":"ONNX to TensorRT","text":"<p>Supported JetPack-ONNX version</p> Target Device JetPack version Input datatype Batch size Channel Input size Output datatype NVIDIA Jetson Nano 4.6, 4.4.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Xavier NX 5.0.2, 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson TX2 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Xavier 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Orin 5.0.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Orin Nano 6.0 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA T4 - FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"ko/launchx/compatibility_range_of_conversion/#onnx-to-tflite","title":"ONNX to TFlite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16INT8"},{"location":"ko/launchx/compatibility_range_of_conversion/#onnx-to-openvino","title":"ONNX to OpenVino","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"ko/launchx/compatibility_range_of_conversion/#tensorflow-to-tensorflowlite","title":"TensorFlow to TensorFlowLite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static, Dynamic) 1~4 height, width FP16INT8"},{"location":"ko/launchx/int8_quantization_with_launchx/","title":"INT8 quantization with LaunchX","text":""},{"location":"ko/launchx/int8_quantization_with_launchx/#what-is-quantization","title":"What is Quantization?","text":"<p>Quantization is a technique that reduces the number of bits used to represent weights and activations in a deep learning model. This makes the model lighter, reducing memory usage and improving inference speed.</p> <p>LaunchX facilitates the easy application of post-training INT8 quantization to the model. It represents the model's weights and activations as 8-bit integers, saving memory and accelerating computations.</p> <p>This document provides information as follows:</p> <ul> <li>INT8 quantization with LaunchX</li> <li>Preparing the calibration dataset</li> <li>Inference code for TFlite INT8 model</li> </ul>"},{"location":"ko/launchx/int8_quantization_with_launchx/#int8-quantization-with-launchx_1","title":"INT8 quantization with LaunchX","text":"<ol> <li>To quantize, upload your target model or select from the free models in Models.</li> <li>On the Convert Setting page, choose Tensorflow Lite, select the target device, and then choose the INT8 option.</li> <li>Upload the calibration dataset(&lt;= 500MB) to minimize quantization error.</li> <li>Click the convert button.</li> </ol>"},{"location":"ko/launchx/int8_quantization_with_launchx/#preparing-the-calibration-dataset","title":"Preparing the calibration dataset","text":"<p>Calibration is the process of aligning a quantized model with a specific data distribution, adjusting quantization parameters such as scaling and zero points.</p> <p>Generally, the calibration dataset is a subset of the original training data, used to fine-tune quantization parameters. Its purpose is to ensure the quantized model's optimal performance on real-world data while maintaining acceptable accuracy. During calibration, the model runs on this dataset, and activation statistics are collected. These statistics determine quantization parameters for each neural network layer, minimizing the impact of quantization on accuracy by adapting parameters based on observed value distributions.</p>"},{"location":"ko/launchx/int8_quantization_with_launchx/#convert-the-dataset-to-numpy-for-calibration","title":"Convert the dataset to NumPy for calibration.","text":"<ul> <li>img_file_dir_path : Path where the dataset for calibration is stored.</li> <li>padding : Value for padding preprocessing.</li> <li>color_mode : Value to change the color mode of an image for preprocessing. Select \u2018bgr\u2019 or \u2018rgb\u2019.</li> <li>normalize : Value for normalization preprocessing.</li> <li>input_shape : Value for width and height of model input shape.</li> <li>save_file_path : Path to save the npy file.</li> </ul> <pre><code>from glob import glob\nimport os\nimport pathlib\nimport random\nfrom typing import Literal, List, Tuple\n\nimport cv2\nimport numpy as np\n\nimg_file_types = [\n    \"*.jpeg\",\n    \"*.JPEG\",\n    \"*.jpg\",\n    \"*.JPG\",\n    \"*.png\",\n    \"*.PNG\",\n    \"*.BMP\",\n    \"*.bmp\",\n    \"*.TIF\",\n    \"*.tif\",\n    \"*.TIFF\",\n    \"*.tiff\",\n    \"*.DNG\",\n    \"*.dng\",\n    \"*.WEBP\",\n    \"*.webp\",\n    \"*.mpo\",\n    \"*.MPO\",\n]\n\ndef save_npy_file(array, save_file_name: str):\n    np.save(save_file_name, array)\n\ndef print_saved_npy_file_shape(save_file_name: str):\n    array = np.load(save_file_name)\n    print(f\"Saved npy array shape is {array.shape}\")\n\ndef save_txt_file(lines: List[str], file_name: str):\n    with open(file_name, \"w\") as file:\n        for line in lines:\n            file.write(line + \"\\n\")\n        file.close()\n\ndef print_error_files(error_files: List[str], file_name: str):\n    print(f\"Please check files in {file_name}.\")\n    print(\"Error files:\", error_files)\n\ndef save_error_files(\n        error_files: List[str],\n        file_name: str = \"error_files.txt\"):\n    if len(error_files) != 0:\n        save_txt_file(error_files, file_name)\n    print_error_files(error_files, file_name)\n\nclass DataPreprocessor():\n    def __init__(\n        self,\n        root_dir: str,\n        input_shape: Tuple[int, int],\n        color_mode: Literal[\"bgr\", \"rgb\"],\n        padding: bool,\n        normalize: bool\n    ):\n        self.root_dir = root_dir\n        self.input_shape = input_shape\n        self.color_mode = color_mode\n        self.padding = padding\n        self.normalize = normalize\n        self.img_list = []\n\n    def _get_img_list(self,):\n        self.img_list = []\n\n        for root, dirs, files in os.walk(self.root_dir):\n            for img_type in img_file_types:\n                img_files = glob(os.path.join(root, img_type))\n                self.img_list += img_files\n\n        self.img_list = list(set(self.img_list))\n        if len(self.img_list) == 0:\n            raise Exception(f\"No image in {self.root_dir}\")\n\n        random.shuffle(self.img_list)\n\n    def _img_padding(self, im):\n        shape = im.shape[:2]  # current shape [height, width]\n        new_shape = self.input_shape\n        if isinstance(new_shape, int):\n            new_shape = (new_shape, new_shape)\n\n        # Scale ratio (new / old)\n        r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n\n        # Compute padding\n        new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n        if shape[::-1] != new_unpad:  # resize\n            im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n        left, right = int(round((new_shape[1] - new_unpad[0])/2 - 0.1)), int(round((new_shape[1] - new_unpad[0])/2 + 0.1))\n        top, bottom = int(round((new_shape[0] - new_unpad[1])/2 - 0.1)), int(round((new_shape[0] - new_unpad[1])/2 + 0.1))\n        im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(114,114,114))  # add border\n        return im\n\n    def _preprocess(self, img):\n        x = []\n        if self.color_mode == \"rgb\":\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.padding:\n            img = self._img_padding(img)\n        else:\n            img = cv2.resize(img, self.input_shape)\n        x.append(np.asarray(img))\n        x = np.array(x)\n        x = np.float32(x)\n        if self.normalize:\n            x = x / 255.0\n        return x\n\n    def _load_dataset(self,):\n        error_files = []\n        valid_images = []\n\n        def is_valid_image(image):\n            return image is not None\n\n        for f in self.img_list:\n            try:\n                img = self._preprocess(cv2.imread(f))\n                if is_valid_image(img):\n                    valid_images.append(img)\n                else:\n                    error_files.append(f)\n            except Exception as e:\n                error_files.append(f)\n        try:\n            result_array = np.concatenate(valid_images, axis=0)\n        except:\n            import pdb; pdb.set_trace()\n        return result_array, error_files\n\n    def _get_save_file_name(self,):\n        self._get_img_list()\n        count = len(self.img_list)\n        w = self.input_shape[0]\n        h = self.input_shape[1]\n        return f\"{count}x{w}x{h}.npy\"\n\n    def save_dataset_as_npy(self, save_file_path: str):\n        save_file_name = self._get_save_file_name()\n        save_file_path = os.path.join(save_file_path, save_file_name)\n        result_array, error_files = self._load_dataset()\n        save_npy_file(result_array, save_file_path)\n        print_saved_npy_file_shape(save_file_path)\n        save_error_files(error_files)\n\nif __name__ == \"__main__\":\n    img_file_dir_path = \"/image/file/path\"\n    padding = True\n    color_mode = \"bgr\"\n    input_shape = (512, 512)\n    normalize = True\n    save_file_path = \"/path/to/save/file\"\n    data_preprocessor = DataPreprocessor(\n        img_file_dir_path,\n        input_shape,\n        color_mode,\n        padding,\n        normalize\n    )\n    data_preprocessor.save_dataset_as_npy(save_file_path)\n</code></pre> <p>Result</p> <pre><code>root@4e54bd5645d1:/app# python3 prepare_npy_file.py \nCorrupt JPEG data: 122 extraneous bytes before marker 0xc4\nSaved npy array shape is (145, 32, 32, 3)\nPlease check files in error_files.txt.\nError files: ['/app/calib_dataset/images/2007_000039_jpg']\n</code></pre>"},{"location":"ko/launchx/int8_quantization_with_launchx/#dataset-that-failed-to-be-read","title":"Dataset that failed to be read","text":"<p>Image files that do not open normally through opencv-python are listed in the 'error_files.txt' created in the code execution location.</p>"},{"location":"ko/launchx/int8_quantization_with_launchx/#inference-code-for-tflite-int8","title":"Inference Code for TFlite INT8","text":"<p>Write and use additional pre-processing/post-processing codes for your model.</p> <pre><code>import os\n\nimport cv2\nimport numpy as np\nimport tensorflow as tf\n\ndef model_input_output_attributes(interpreter: tf.lite.Interpreter):\n    inputs = {}\n    outputs = {}\n    for input_detail in interpreter.get_input_details():\n        input_data_attribute = {}\n        input_data_attribute[\"name\"] = input_detail.get(\"name\")\n        input_data_attribute[\"location\"] = input_detail.get(\"index\")\n        input_data_attribute[\"shape\"] = tuple(input_detail.get(\"shape\"))\n        input_data_attribute[\"dtype\"] = input_detail.get(\"dtype\")\n        input_data_attribute[\"quantization\"] = input_detail.get(\"quantization\")\n        input_data_attribute[\"format\"] = \"nchw\" if input_data_attribute[\"shape\"][1] == 3 else \"nhwc\"\n        inputs[input_data_attribute[\"location\"] if input_data_attribute[\"location\"] is not None else input_data_attribute[\"name\"]] = input_data_attribute\n\n    for output_detail in interpreter.get_output_details():\n        output_data_attribute = {}\n        output_data_attribute[\"name\"] = output_detail.get(\"name\")\n        output_data_attribute[\"location\"] = output_detail.get(\"index\")\n        output_data_attribute[\"shape\"] = tuple(output_detail.get(\"shape\"))\n        output_data_attribute[\"dtype\"] = output_detail.get(\"dtype\")\n        output_data_attribute[\"quantization\"] = output_detail.get(\"quantization\")\n        output_data_attribute[\"format\"] = \"nchw\" if output_data_attribute[\"shape\"][1] == 3 else \"nhwc\"\n        outputs[output_data_attribute[\"location\"] if output_data_attribute[\"location\"] is not None else output_data_attribute[\"name\"]] = output_data_attribute\n\n    return inputs, outputs\n\ndef preprocess(input_image_path:str):\n    input_image = cv2.imread(input_image_path)\n    # Write down the pre-processing process required for your model, including resizing.\n    return input_image\n\ndef inference(tflite_model_path:str, input_image):\n    with open(os.path.join(tflite_model_path), \"rb\") as f:\n        interpreter = tf.lite.Interpreter(model_content=f.read(), num_threads=1)\n    interpreter.allocate_tensors()\n\n    inputs, outputs = model_input_output_attributes(interpreter)\n    for k, v in inputs.items():\n        if v['dtype'] in [np.uint8, np.int8]:\n            input_scale, input_zero_point = v[\"quantization\"]\n            input_image = input_image / input_scale + input_zero_point\n            input_image = np.expand_dims(input_image, axis=0).astype(v[\"dtype\"])\n        interpreter.set_tensor(v[\"location\"], input_image)\n    interpreter.invoke()\n\n    output_dict = {}\n    for output_location in iter(outputs):\n        output_dict[output_location] = interpreter.get_tensor(output_location)\n\n    return output_dict\n\ndef postprocess(inference_results):\n    return inference_results\n\ninput_image=preprocess(\"your_image_file.jpg\")\ninference_result=inference(\"your_model.tflite\", input_image)\npostprocess(inference_result)\n</code></pre>"},{"location":"ko/launchx/supported_jetpack_onnx_version/","title":"Supported JetPack-ONNX version","text":"JetPack \bonnxruntime ONNX ONNX opset version ONNX ML opset version ONNX IR version 4.4 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.4.1 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.6 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 5.0.1 1.12.1 1.12 17 3 8 5.0.2 1.12.1 1.12 17 3 8 6.0 1.17.0 1.15 20 4 9"}]}