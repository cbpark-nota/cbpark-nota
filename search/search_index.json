{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NetsPresso Documentation","text":"<p>Nota has built NetsPresso to help other developers to make custom AI models faster and easier for edge devices. NetsPresso aims to provide critical features to offer hands-on experience with hardware-aware autoML, optimization, and device farm to explore possibilities with NetsPresso.</p> <p>This document includes product configuration, benchmarks, and other important information to help you during using NetsPresso. Please feel free to compare the results obtained using Nota NetsPresso with other model architecture optimization frameworks. We are constantly striving to keep up with the latest AI model optimization technology and we welcome your feedback!</p>"},{"location":"#about-nota","title":"About Nota","text":"<p>In 2015, we started with a mobile keyboard app that prevents typos. During the app development, we realized how painful it was to optimize the deep learning model on a tiny mobile device. That experience lead us to the decision to share our know-hows with other developers not only to build, but optimize models per a particular hardware. Since then, Nota has focused on deep learning optimization technology and will continue to develop its AI software optimization platform to support AI developer community. Nota has offices in South Korea, US, and Germany and partners with over 35 global market leaders from a wide range of industries including system integrator, device manufacturers, platform providers, and mobility.</p> <p>\ud83c\udfe0\u00a0More about Nota</p>"},{"location":"#contact-us","title":"Contact us","text":"<p>If you have any questions or comments, please contact us.</p> <p>We look forward to your feedback.</p> <p>\ud83d\ude4f Leave Feedbacks:\u00a0NetsPresso Discussion Forum</p> <p>\ud83d\udc68\u200d\ud83d\udcbb NetsPresso Support Center:\u00a0netspresso@nota.ai</p>"},{"location":"en/launchx/","title":"LaunchX","text":"<p>LaunchX is a web application that facilitates the essential tasks of framework conversion and benchmarking for deploying AI models on devices quickly and easily. It allows AI developers, researchers, and even non-coders to intuitively use the ultimate converter and benchmarker. If you are a Python user, you can access the same functionality as a Python package through NetsPresso.</p>"},{"location":"en/launchx/#models","title":"Models","text":"<p>We provide optimized models through NetsPresso. You can immediately utilize these AI models after converting them.</p> <ul> <li>Utilize a variety of computer vision task models for free.</li> <li>You can download converted models.</li> <li>You can request customization for a fine-tuned model that is tailored to your project environment.</li> </ul>"},{"location":"en/launchx/#convert","title":"Convert","text":"<p>Simplify the process of framework conversion.</p> <p></p> <ul> <li>Easy and efficient model conversion.</li> <li>Eliminate trial and error with automated conversion.</li> <li>Quickly set up the deployment of models on target devices.</li> </ul> <p>TODO: link change See supported frameworks</p>"},{"location":"en/launchx/#benchmark","title":"Benchmark","text":"<p>Easily find the right device for your AI model.</p> <p></p> <ul> <li>Identify the best-fit device through benchmarking.</li> <li>Check model compatibility with target devices.</li> <li>Evaluate real-device performance without acquiring or setting up the device.</li> </ul> <p>TODO: link change View compatible devices</p>"},{"location":"en/launchx/#request-your-device","title":"Request your device","text":"<p>We continuously research and develop to optimize AI models whenever new devices are released through partnerships with device manufacturers. If you have a specific device you'd like to benchmark, please make a request to us.</p>"},{"location":"en/launchx/#we-hear-from-you","title":"We hear from you","text":"<p>If you encounter difficulties or need additional credits, feel free to contact us anytime!</p> <p>\ud83d\udc68\u200d\ud83d\udcbb NetsPresso Support Center:\u00a0netspresso@nota.ai</p>"},{"location":"en/launchx/compatibility_range_of_conversion/","title":"Compatibility range of conversion","text":"<p>You can use LaunchX converter to automatically convert the AI model's framework to the target framework.</p>"},{"location":"en/launchx/compatibility_range_of_conversion/#converting-case","title":"Converting case","text":"<ol> <li>ONNX to TensorRT</li> <li>ONNX to TensorFlow Lite</li> <li>ONNX to OpenVINO</li> <li>TensorFlow-Keras to TensorFlow Lite</li> </ol>"},{"location":"en/launchx/compatibility_range_of_conversion/#compatible-model","title":"Compatible model","text":"<p>The input layer of the uploaded model should be as follows.</p> <p></p> <ul> <li>Only single-input models are supported.</li> <li>The four-dimensional array structure of images should be organized Batch, Number of Channels, Height, and Width.</li> </ul> <ol> <li>Batch size: The number of combined input datasets that the model processes simultaneously.</li> <li>Channel: 3 for RGB or BGR and 1 for Grayscale.</li> <li>Input size: In computer vision tasks, input size refers to the size of the input images. </li> </ol>"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-tensorrt","title":"ONNX to TensorRT","text":"<p>Supported JetPack-ONNX version</p> Target Device JetPack version Input datatype Batch size Channel Input size Output datatype NVIDIA Jetson Nano 4.6, 4.4.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Xavier NX 5.0.2, 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson TX2 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Xavier 4.6 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson AGX Orin 5.0.1 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA Jetson Orin Nano 6.0 FP32 1~4 (Static), Dynamic 1~4 height, width FP16 NVIDIA T4 - FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-tflite","title":"ONNX to TFlite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16INT8"},{"location":"en/launchx/compatibility_range_of_conversion/#onnx-to-openvino","title":"ONNX to OpenVino","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static), Dynamic 1~4 height, width FP16"},{"location":"en/launchx/compatibility_range_of_conversion/#tensorflow-to-tensorflowlite","title":"TensorFlow to TensorFlowLite","text":"Input datatype Batch size Channel Input size Output datatype FP32 1~4 (Static, Dynamic) 1~4 height, width FP16INT8"},{"location":"en/launchx/supported_jetpack_onnx_version/","title":"Supported JetPack-ONNX version","text":"JetPack \bonnxruntime ONNX ONNX opset version ONNX ML opset version ONNX IR version 4.4 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.4.1 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 4.6 1.4.0 1.7 12 2 7 1.5.2 1.7 12 2 7 1.6.0 1.8 13 2 7 1.7.0 1.8 13 2 7 1.8.0 1.9 14 2 7 1.9.0 1.10 15 2 8 1.10.0 1.10 15 2 8 1.11.0 1.11 16 2 8 5.0.1 1.12.1 1.12 17 3 8 5.0.2 1.12.1 1.12 17 3 8 6.0 1.17.0 1.15 20 4 9"}]}